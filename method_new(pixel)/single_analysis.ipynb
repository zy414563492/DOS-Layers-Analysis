{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "import scipy.optimize\n",
    "from sklearn import mixture\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot single gaussian\n",
    "Use six parameters to plot gaussian.\n",
    "$$\n",
    "p = [A, x_0, y_0, \\sigma_x ^2, \\sigma_y ^2, \\theta]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaussian(g_size, p):\n",
    "    gaussian = np.zeros((g_size, g_size))\n",
    "\n",
    "    rotation_mat = np.array([[np.cos(p[5]), -np.sin(p[5])], [np.sin(p[5]), np.cos(p[5])]])\n",
    "    center_vert = np.array([p[1], p[2]]).reshape((-1, 1))\n",
    "\n",
    "    for x in range(g_size):\n",
    "        for y in range(g_size):\n",
    "            point = np.array([x, y]).reshape((-1, 1))\n",
    "            transrated_point = np.dot(rotation_mat, (point - center_vert))\n",
    "\n",
    "            gaussian[x, y] = p[0] * np.exp((-1 / 2) * ((transrated_point[0] / p[3]) ** 2 + (transrated_point[1] / p[4]) ** 2))\n",
    "\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot double gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaussians2_sum(num_gaus, g_size, p):\n",
    "    gaussian = np.zeros((g_size, g_size))\n",
    "\n",
    "    for n in range(num_gaus):\n",
    "        gaussian += make_gaussian(g_size, p[n * 6: (n + 1) * 6])\n",
    "\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning gaussian parameters\n",
    "Use four parameters to tuning the given gaussian parameters.\n",
    "$$\n",
    "p = [scale, \\theta\\_all, trans\\_x, trans\\_y]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuned_gaussians(g_size, p, x):\n",
    "    if x[0] != 1.8:\n",
    "        return np.ones((g_size, g_size))\n",
    "    param_new = fine_tuning(g_size, p, x[0], x[1], x[2], x[3])\n",
    "    # print(\"x:\", x)\n",
    "    new_size = int(g_size * x[0])\n",
    "    tuned_img = make_gaussians2_sum(2, new_size, param_new)\n",
    "    tuned_gaussian = cv2.resize(tuned_img, (g_size, g_size), interpolation=cv2.INTER_AREA)\n",
    "    return tuned_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(g_size, p, scale, theta_all, trans_x, trans_y):\n",
    "    k = p.copy()\n",
    "    x_mid = (k[1] + k[7]) / 2\n",
    "    y_mid = (k[2] + k[8]) / 2\n",
    "    x1_new = np.array(k[1] - x_mid) * np.cos(theta_all) - np.array(k[2] - y_mid) * np.sin(theta_all) + x_mid\n",
    "    y1_new = np.array(k[1] - x_mid) * np.sin(theta_all) + np.array(k[2] - y_mid) * np.cos(theta_all) + y_mid\n",
    "    x2_new = np.array(k[7] - x_mid) * np.cos(theta_all) - np.array(k[8] - y_mid) * np.sin(theta_all) + x_mid\n",
    "    y2_new = np.array(k[7] - x_mid) * np.sin(theta_all) + np.array(k[8] - y_mid) * np.cos(theta_all) + y_mid\n",
    "    k[1] = x1_new\n",
    "    k[2] = y1_new\n",
    "    k[7] = x2_new\n",
    "    k[8] = y2_new\n",
    "    k[1] += g_size * (scale - 1) / 2\n",
    "    k[2] += g_size * (scale - 1) / 2\n",
    "    k[7] += g_size * (scale - 1) / 2\n",
    "    k[8] += g_size * (scale - 1) / 2\n",
    "    k[1] += trans_x * scale\n",
    "    k[2] += trans_y * scale\n",
    "    k[7] += trans_x * scale\n",
    "    k[8] += trans_y * scale\n",
    "    k[5] -= theta_all\n",
    "    k[11] -= theta_all\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(columns=['score', 'scale', 'theta_all', 'trans_x', 'trans_y', 'pro1', 'pro2'])\n",
    "\n",
    "tgt_layer = 'convolution_7'\n",
    "\n",
    "units = list(range(0, 512))\n",
    "u_num = len(units)\n",
    "\n",
    "data_all = np.zeros((u_num, 128, 128, 3))\n",
    "data_black = np.zeros((128, 128, 3))\n",
    "for i in range(u_num):\n",
    "    data_all[i] = pd.read_pickle(\"./visualize_laplacian/7-15/visualize-lap-{}-{}-15.pkl\".format(tgt_layer, units[i]))[64:192, 64:192]\n",
    "\n",
    "# If you want do complete calculation, just change the range to (0, 512)\n",
    "for unit_use in range(233, 234):\n",
    "    print(\"unit_use\", unit_use)\n",
    "    data_row = data_all[unit_use]\n",
    "    data_row = np.clip(data_row, 0., 1.)\n",
    "\n",
    "    ##################### ↓↓↓ CNN feature map processing ↓↓↓ #####################\n",
    "\n",
    "    # Use Gaussian blur to remove some noise\n",
    "    data_blur = cv2.GaussianBlur(data_row, (3, 3), 0)\n",
    "\n",
    "    # Find the central region\n",
    "    image_array = data_blur.reshape((data_blur.shape[0] * data_blur.shape[1], 3))\n",
    "\n",
    "    # 1st: GMM cluster\n",
    "    clst = mixture.GaussianMixture(n_components=3, max_iter=100, covariance_type=\"full\")\n",
    "    clst.fit(image_array)\n",
    "    predicted_labels = clst.predict(image_array)\n",
    "    centroids_GMM = clst.means_\n",
    "#     print(\"centroids_GMM\", centroids_GMM)\n",
    "    labels = predicted_labels\n",
    "    label_matrix = labels.reshape((128, 128))\n",
    "\n",
    "    baseline = np.array([0.5] * 9).reshape(3, 3)\n",
    "    max_index = np.argmax(np.sum(abs(centroids_GMM - baseline), axis=1))\n",
    "    center_mean = centroids_GMM[max_index].dot(np.array([0.2989, 0.5870, 0.1140]).T)\n",
    "#     print(\"center_mean:\", center_mean)\n",
    "\n",
    "    # Get the grayscale map, but we still need to do further preprocessing.\n",
    "    data_filled = data_blur.dot(np.array([0.2989, 0.5870, 0.1140]).T) - center_mean\n",
    "\n",
    "    data_center = data_blur.copy()\n",
    "    if max_index == 0:\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                if label_matrix[i, j] != 0:\n",
    "                    data_center[i][j] = [center_mean, center_mean, center_mean]\n",
    "    elif max_index == 1:\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                if label_matrix[i, j] != 1:\n",
    "                    data_center[i][j] = [center_mean, center_mean, center_mean]\n",
    "    else:\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                if label_matrix[i, j] != 2:\n",
    "                    data_center[i][j] = [center_mean, center_mean, center_mean]\n",
    "\n",
    "    data_filled_center = data_center.dot(np.array([0.2989, 0.5870, 0.1140]).T) - center_mean\n",
    "\n",
    "    image_array_new = data_center.reshape((data_row.shape[0] * data_row.shape[1], 3))\n",
    "\n",
    "    # 2nd: k-means cluster\n",
    "    kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "    kmeans.fit(image_array_new)\n",
    "    centroids_kmeans = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "#     print(\"centroids_k-means\", centroids_kmeans)\n",
    "    label_matrix_new = labels.reshape((128, 128))\n",
    "\n",
    "    data_0_new = data_center.copy()\n",
    "    data_1_new = data_center.copy()\n",
    "    data_2_new = data_center.copy()\n",
    "    for i in range(128):\n",
    "        for j in range(128):\n",
    "            if label_matrix_new[i, j] == 0:\n",
    "                data_0_new[i][j] = [1, 1, 1]\n",
    "            else:\n",
    "                data_0_new[i][j] = [0, 0, 0]\n",
    "            if label_matrix_new[i, j] == 1:\n",
    "                data_1_new[i][j] = [1, 1, 1]\n",
    "            else:\n",
    "                data_1_new[i][j] = [0, 0, 0]\n",
    "            if label_matrix_new[i, j] == 2:\n",
    "                data_2_new[i][j] = [1, 1, 1]\n",
    "            else:\n",
    "                data_2_new[i][j] = [0, 0, 0]\n",
    "\n",
    "    # Determine the facilitative and suppressive region\n",
    "    centroids_filled = centroids_kmeans.dot(np.array([0.2989, 0.5870, 0.1140]).T)\n",
    "    pos_index = np.argmax(centroids_filled)\n",
    "    neg_index = np.argmin(centroids_filled)\n",
    "#     print(\"pos_index, neg_index = \", pos_index, neg_index)\n",
    "    data_pos = data_0_new if pos_index == 0 else (data_1_new if pos_index == 1 else data_2_new)\n",
    "    data_neg = data_0_new if neg_index == 0 else (data_1_new if neg_index == 1 else data_2_new)\n",
    "\n",
    "    # Erosion and dilation\n",
    "    kernel = np.ones((4, 4), np.uint8)\n",
    "    data_pos = cv2.morphologyEx(cv2.morphologyEx(data_pos, cv2.MORPH_CLOSE, kernel), cv2.MORPH_OPEN, kernel)\n",
    "    data_neg = cv2.morphologyEx(cv2.morphologyEx(data_neg, cv2.MORPH_CLOSE, kernel), cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    data_mod = data_filled_center.copy()\n",
    "    for i in range(128):\n",
    "        for j in range(128):\n",
    "            if (data_pos[i, j] == [1, 1, 1]).all() and (data_neg[i, j] != [1, 1, 1]).all():\n",
    "                data_mod[i][j] = abs(data_mod[i][j])\n",
    "            elif (data_neg[i, j] == [1, 1, 1]).all() and (data_pos[i, j] != [1, 1, 1]).all():\n",
    "                data_mod[i][j] = -abs(data_mod[i][j])\n",
    "            elif (data_pos[i, j] == [1, 1, 1]).all() and (data_neg[i, j] == [1, 1, 1]).all():\n",
    "                data_mod[i][j] = data_mod[i][j] / 2\n",
    "\n",
    "    # Average blur\n",
    "    data_mod = cv2.blur(data_mod, (2, 2))\n",
    "\n",
    "    score_mod = np.sqrt(np.sum(data_mod ** 2))\n",
    "    print(\"score_mod = \", score_mod)\n",
    "\n",
    "    ##################### ↑↑↑ CNN feature map processing ↑↑↑ #####################\n",
    "\n",
    "    n_pos = 1\n",
    "    n_neg = 1\n",
    "\n",
    "    stim_size = 128\n",
    "    param_init = [0.4356, 78.61, 64.08, 19.72, 12.39, 1.477,\n",
    "                  -0.38804, 48.145, 64.923, 32, 32, 0.089]\n",
    "\n",
    "    # Set constraint\n",
    "    # min_scale = 1.5\n",
    "    # max_scale = 2.0\n",
    "    min_theta = -np.pi\n",
    "    max_theta = np.pi\n",
    "    # min_trans = -int(stim_size / 6)\n",
    "    # max_trans = int(stim_size / 6)\n",
    "    x_trans = -5\n",
    "    y_trans = 1\n",
    "\n",
    "\n",
    "    objfcn = lambda x: np.sqrt(np.sum((data_mod - make_tuned_gaussians(stim_size, param_init, x)) ** 2))\n",
    "\n",
    "\n",
    "    # start fitting\n",
    "    score = np.inf\n",
    "    param = np.zeros(4)\n",
    "    max_itr = 5\n",
    "    for itr in range(max_itr * (n_pos + n_neg)):\n",
    "        print(\"itr:\", itr)\n",
    "\n",
    "        # Set initial value by random\n",
    "        init_x = np.array([1.8,  # (max_scale - min_scale) * random.random() + min_scale,\n",
    "                           (max_theta - min_theta) * random.random() + min_theta,\n",
    "                           x_trans,  # (max_trans - min_trans) * random.random() + min_trans,\n",
    "                           y_trans])  # (max_trans - min_trans) * random.random() + min_trans])\n",
    "\n",
    "        # SLSQP constraint\n",
    "        cons = ({'type': 'eq', 'fun': lambda x: x[0] - 1.8},  # {'type': 'ineq', 'fun': lambda x: x[0] - min_scale}, {'type': 'ineq', 'fun': lambda x: -x[0] + max_scale},\n",
    "                {'type': 'ineq', 'fun': lambda x: x[1] - min_theta}, {'type': 'ineq', 'fun': lambda x: -x[1] + max_theta},\n",
    "                {'type': 'eq', 'fun': lambda x: x[2] - x_trans},   # {'type': 'ineq', 'fun': lambda x: x[2] - min_trans}, {'type': 'ineq', 'fun': lambda x: -x[2] + max_trans},\n",
    "                {'type': 'eq', 'fun': lambda x: x[3] - y_trans})  # {'type': 'ineq', 'fun': lambda x: x[3] - min_trans}, {'type': 'ineq', 'fun': lambda x: -x[3] + max_trans})\n",
    "\n",
    "        res = scipy.optimize.minimize(objfcn, init_x, method='SLSQP', constraints=cons)\n",
    "\n",
    "        cur_success = res.success\n",
    "        cur_score = res.fun\n",
    "        cur_param = res.x\n",
    "        print(\"cur_success:\", res.success)\n",
    "        print(\"cur_score:\", cur_score)\n",
    "        print(\"cur_param:\", cur_param)\n",
    "\n",
    "        # Get the lowest score, which means the least difference between initial data and its fitting gaussian figure.\n",
    "        if cur_score < score and cur_success == True:\n",
    "            score = cur_score\n",
    "            param = cur_param\n",
    "\n",
    "    print(\"score:\", score)\n",
    "    print(\"param:\", param)\n",
    "\n",
    "    score_gaussian = np.sqrt(np.sum(make_tuned_gaussians(stim_size, param_init, param) ** 2))\n",
    "    print(\"score_gaussian:\", score_gaussian)\n",
    "    pro1 = score / score_gaussian\n",
    "    pro2 = score / score_mod\n",
    "    print(\"pro1 = {}, pro2 = {}\".format(pro1, pro2))\n",
    "\n",
    "\n",
    "    result_list = [score, param[0], param[1], param[2], param[3], pro1, pro2]\n",
    "    dataframe = dataframe.append(pd.Series(result_list, ['score', 'scale', 'theta_all', 'trans_x', 'trans_y', 'pro1', 'pro2']), ignore_index=True)\n",
    "\n",
    "dataframe.to_csv('tuning_param_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
